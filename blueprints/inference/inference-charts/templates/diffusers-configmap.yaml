{{- if eq .Values.inference.framework "diffusers" }}
apiVersion: v1
kind: ConfigMap
metadata:
  name: diffusers-serve
  labels:
    "app.kubernetes.io/component": "{{.Values.inference.serviceName}}"
data:
  serve_script.py: |
    import torch
    import io
    from fastapi.responses import StreamingResponse
    import os
    import logging
    import uvicorn
    from pydantic import BaseModel
    from fastapi import FastAPI
    from huggingface_hub import login
    import importlib

    # Environment and configuration setup
    logger = logging.getLogger("serve")
    app = FastAPI()

    class Request(BaseModel):
      prompt: str

    class DiffuserDeployment:
      def __init__(self):
        self.model_id = os.getenv("MODEL_ID")
        self.pipeline_name = os.getenv("PIPELINE").lower()

        if not self.model_id:
            raise ValueError("MODEL_ID environment variable is required")
        if not self.pipeline_name:
            raise ValueError("PIPELINE environment variable is required")

        self.pipe = self.get_pipeline()

        hf_token = os.getenv("HUGGING_FACE_HUB_TOKEN")
        if not hf_token:
          logger.warning("HUGGING_FACE_HUB_TOKEN environment variable is not set")
        else:
          login(token=hf_token)
          logger.info("Successfully logged in to Hugging Face Hub")

      def get_pipeline_config(self, pipeline_name):
          """Get pipeline configuration by name"""
          configs = {
              "flux": {
                  "class": "FluxPipeline",
                  "torch_dtype": torch.bfloat16,
                  "variant": None
              },
              "stablediffusion3": {
                  "class": "StableDiffusion3Pipeline",
                  "torch_dtype": torch.bfloat16,
                  "variant": None
              },
              "kolors": {
                  "class": "KolorsPipeline",
                  "torch_dtype": torch.float16,
                  "variant": "fp16"
              },
              "omnigen": {
                  "class": "OmniGenPipeline",
                  "torch_dtype": torch.bfloat16,
                  "variant": None
              },
              "diffusion": {
                  "class": "DiffusionPipeline",
                  "torch_dtype": None,
                  "variant": None
              }
          }

          if pipeline_name not in configs:
              available = ", ".join(configs.keys())
              raise ValueError(f"Unknown pipeline '{pipeline_name}'. Available: {available}")

          return configs[pipeline_name]

      def get_pipeline(self):
          """Load pipeline using explicit environment variable"""
          logger.info(f"Loading pipeline '{self.pipeline_name}' for model '{self.model_id}'")

          config = self.get_pipeline_config(self.pipeline_name)

          # Dynamically import the pipeline class
          diffusers_module = importlib.import_module("diffusers")
          pipeline_class = getattr(diffusers_module, config["class"])

          # Build kwargs for pipeline initialization
          kwargs = {}
          if config["torch_dtype"]:
              kwargs["torch_dtype"] = config["torch_dtype"]
          if config["variant"]:
              kwargs["variant"] = config["variant"]

          logger.info(f"Initializing {config['class']} with model {self.model_id}")
          return pipeline_class.from_pretrained(self.model_id, **kwargs).to("cuda")
      async def get_response(self, raw_request: Request):
                                                  prompt = raw_request.prompt
                                                  image = self.pipe(prompt).images[0]
                                                  memory_stream = io.BytesIO()
                                                  image.save(memory_stream, format="PNG")
                                                  memory_stream.seek(0)
                                                  return StreamingResponse(memory_stream, media_type="image/png")
    def create_deployment():
      global deployment
      deployment = DiffuserDeployment()
      return deployment

    @app.on_event("startup")
    async def startup_event():
      create_deployment()

    @app.post("/v1/generations")
    async def create_completion(raw_request: Request):
                                               return await deployment.get_response(raw_request)

    if __name__ == "__main__":
      uvicorn.run(app, host="0.0.0.0", port=8000)
{{- end }}
