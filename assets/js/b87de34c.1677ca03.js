"use strict";(self.webpackChunkdoeks_website=self.webpackChunkdoeks_website||[]).push([[3887],{28453:(e,n,l)=>{l.d(n,{R:()=>c,x:()=>d});var r=l(96540);const s={},i=r.createContext(s);function c(e){const n=r.useContext(i);return r.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function d(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(s):e.components||s:c(e.components),r.createElement(i.Provider,{value:n},e.children)}},89220:(e,n,l)=>{l.r(n),l.d(n,{assets:()=>o,contentTitle:()=>d,default:()=>h,frontMatter:()=>c,metadata:()=>r,toc:()=>a});const r=JSON.parse('{"id":"blueprints/inference/inference-charts","title":"AI on EKS Inference Charts","description":"The AI on EKS Inference Charts provide a streamlined Helm-based approach to deploy AI/ML inference workloads on both GPU and AWS Neuron (Inferentia/Trainium) hardware. This chart supports multiple deployment configurations and comes with pre-configured values for popular models.","source":"@site/docs/blueprints/inference/inference-charts.md","sourceDirName":"blueprints/inference","slug":"/blueprints/inference/inference-charts","permalink":"/ai-on-eks/docs/blueprints/inference/inference-charts","draft":false,"unlisted":false,"editUrl":"https://github.com/awslabs/ai-on-eks/blob/main/website/docs/blueprints/inference/inference-charts.md","tags":[],"version":"current","frontMatter":{"sidebar_label":"Inference Charts"},"sidebar":"blueprints","previous":{"title":"Overview","permalink":"/ai-on-eks/docs/blueprints/inference/"}}');var s=l(74848),i=l(28453);const c={sidebar_label:"Inference Charts"},d="AI on EKS Inference Charts",o={},a=[{value:"Overview",id:"overview",level:2},{value:"Prerequisites",id:"prerequisites",level:2},{value:"Quick Start",id:"quick-start",level:2},{value:"1. Create Hugging Face Token Secret",id:"1-create-hugging-face-token-secret",level:3},{value:"2. Deploy a Pre-configured Model",id:"2-deploy-a-pre-configured-model",level:3},{value:"Supported Models",id:"supported-models",level:2},{value:"Language Models",id:"language-models",level:3},{value:"Diffusion Models",id:"diffusion-models",level:3},{value:"Neuron-Optimized Models",id:"neuron-optimized-models",level:3},{value:"Deployment Examples",id:"deployment-examples",level:2},{value:"Language Model Deployments",id:"language-model-deployments",level:3},{value:"Diffusion Model Deployments",id:"diffusion-model-deployments",level:3},{value:"Neuron Deployments",id:"neuron-deployments",level:3},{value:"Configuration",id:"configuration",level:2},{value:"Key Parameters",id:"key-parameters",level:3},{value:"Custom Configuration",id:"custom-configuration",level:3},{value:"API Usage",id:"api-usage",level:2},{value:"VLLM/Ray-VLLM",id:"vllmray-vllm",level:3},{value:"Triton-VLLM",id:"triton-vllm",level:3},{value:"Diffusers",id:"diffusers",level:3},{value:"Example Usage",id:"example-usage",level:3},{value:"Troubleshooting",id:"troubleshooting",level:2},{value:"Common Issues",id:"common-issues",level:3},{value:"Logs",id:"logs",level:3},{value:"Check Logs",id:"check-logs",level:3},{value:"Next Steps",id:"next-steps",level:2}];function t(e){const n={a:"a",admonition:"admonition",code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,i.R)(),...e.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(n.header,{children:(0,s.jsx)(n.h1,{id:"ai-on-eks-inference-charts",children:"AI on EKS Inference Charts"})}),"\n",(0,s.jsx)(n.p,{children:"The AI on EKS Inference Charts provide a streamlined Helm-based approach to deploy AI/ML inference workloads on both GPU and AWS Neuron (Inferentia/Trainium) hardware. This chart supports multiple deployment configurations and comes with pre-configured values for popular models."}),"\n",(0,s.jsx)(n.admonition,{title:"Advanced Usage",type:"info",children:(0,s.jsxs)(n.p,{children:["For detailed configuration options, advanced deployment scenarios, and comprehensive parameter documentation, see the ",(0,s.jsx)(n.a,{href:"https://github.com/awslabs/ai-on-eks/blob/main/blueprints/inference/inference-charts/README.md",children:"complete README"}),"."]})}),"\n",(0,s.jsx)(n.h2,{id:"overview",children:"Overview"}),"\n",(0,s.jsx)(n.p,{children:"The inference charts support multiple deployment frameworks:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"VLLM"})," - Single-node inference with fast startup"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Ray-VLLM"})," - Distributed inference with autoscaling capabilities"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Triton-VLLM"})," - Production-ready inference server with advanced features"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"AIBrix"})," - VLLM with AIBrix-specific configurations"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"LeaderWorkerSet-VLLM"})," - Multi-node inference for large models"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Diffusers"})," - Hugging Face Diffusers for image generation"]}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:"Both GPU and AWS Neuron (Inferentia/Trainium) accelerators are supported across these frameworks."}),"\n",(0,s.jsx)(n.h2,{id:"prerequisites",children:"Prerequisites"}),"\n",(0,s.jsx)(n.p,{children:"Before deploying the inference charts, ensure you have:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:["Amazon EKS cluster with GPU or AWS Neuron\nnodes (",(0,s.jsx)(n.a,{href:"/ai-on-eks/docs/infra/inference-ready-cluster",children:"inference-ready cluster"})," for a quick start)"]}),"\n",(0,s.jsx)(n.li,{children:"Helm 3.0+"}),"\n",(0,s.jsx)(n.li,{children:"For GPU deployments: NVIDIA device plugin installed"}),"\n",(0,s.jsx)(n.li,{children:"For Neuron deployments: AWS Neuron device plugin installed"}),"\n",(0,s.jsx)(n.li,{children:"For LeaderWorkerSet deployments: LeaderWorkerSet CRD installed"}),"\n",(0,s.jsxs)(n.li,{children:["Hugging Face Hub token (stored as a Kubernetes secret named ",(0,s.jsx)(n.code,{children:"hf-token"}),")"]}),"\n",(0,s.jsx)(n.li,{children:"For Ray: KubeRay Infrastructure"}),"\n",(0,s.jsx)(n.li,{children:"For AIBrix: AIBrix Infrastructure"}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"quick-start",children:"Quick Start"}),"\n",(0,s.jsx)(n.h3,{id:"1-create-hugging-face-token-secret",children:"1. Create Hugging Face Token Secret"}),"\n",(0,s.jsxs)(n.p,{children:["Create a Kubernetes secret with your ",(0,s.jsx)(n.a,{href:"https://huggingface.co/docs/hub/en/security-tokens",children:"Hugging Face token"}),":"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"kubectl create secret generic hf-token --from-literal=token=your_huggingface_token\n"})}),"\n",(0,s.jsx)(n.h3,{id:"2-deploy-a-pre-configured-model",children:"2. Deploy a Pre-configured Model"}),"\n",(0,s.jsx)(n.p,{children:"Choose from the available pre-configured models and deploy:"}),"\n",(0,s.jsx)(n.admonition,{type:"warning",children:(0,s.jsxs)(n.p,{children:["These deployments will need GPU/Neuron resources which need to\nbe ",(0,s.jsx)(n.a,{href:"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-resource-limits.html",children:"enabled"})," and cost more than CPU only\ninstances."]})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"# Deploy Llama 3.2 1B on GPU with vLLM\nhelm install llama-inference ./blueprints/inference/inference-charts \\\n  --values ./blueprints/inference/inference-charts/values-llama-32-1b-vllm.yaml\n\n# Deploy DeepSeek R1 Distill on GPU with Ray-vLLM\nhelm install deepseek-inference ./blueprints/inference/inference-charts \\\n  --values ./blueprints/inference/inference-charts/values-deepseek-r1-distill-llama-8b-ray-vllm-gpu.yaml\n"})}),"\n",(0,s.jsx)(n.h2,{id:"supported-models",children:"Supported Models"}),"\n",(0,s.jsx)(n.p,{children:"The inference charts include pre-configured values files for popular models across different categories:"}),"\n",(0,s.jsx)(n.h3,{id:"language-models",children:"Language Models"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"DeepSeek R1 Distill Llama 8B"})," - Advanced reasoning model"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Llama 3.2 1B"})," - Lightweight language model"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Llama 4 Scout 17B"})," - Mid-size language model"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Mistral Small 24B"})," - Efficient large language model"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"GPT OSS 20B"})," - Open-source GPT variant"]}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"diffusion-models",children:"Diffusion Models"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"FLUX.1 Schnell"})," - Fast text-to-image generation"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Stable Diffusion XL"})," - High-quality image generation"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Stable Diffusion 3.5"})," - Latest SD model with enhanced capabilities"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Kolors"})," - Artistic image generation"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"OmniGen"})," - Multi-modal generation"]}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"neuron-optimized-models",children:"Neuron-Optimized Models"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Llama 2 13B"})," - Optimized for AWS Inferentia"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Llama 3 70B"})," - Large model on Inferentia"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Llama 3.1 8B"})," - Efficient Inferentia deployment"]}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:"Each model comes with optimized configurations for different frameworks (VLLM, Ray-VLLM, Triton-VLLM, etc.)."}),"\n",(0,s.jsx)(n.h2,{id:"deployment-examples",children:"Deployment Examples"}),"\n",(0,s.jsx)(n.h3,{id:"language-model-deployments",children:"Language Model Deployments"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"# Deploy Llama 3.2 1B with VLLM\nhelm install llama32-vllm ./blueprints/inference/inference-charts \\\n  --values ./blueprints/inference/inference-charts/values-llama-32-1b-vllm.yaml\n\n# Deploy DeepSeek R1 Distill with Ray-VLLM\nhelm install deepseek-ray ./blueprints/inference/inference-charts \\\n  --values ./blueprints/inference/inference-charts/values-deepseek-r1-distill-llama-8b-ray-vllm-gpu.yaml\n\n# Deploy Llama 4 Scout 17B with LeaderWorkerSet-VLLM\nhelm install llama4-lws ./blueprints/inference/inference-charts \\\n  --values ./blueprints/inference/inference-charts/values-llama-4-scout-17b-lws-vllm.yaml\n"})}),"\n",(0,s.jsx)(n.h3,{id:"diffusion-model-deployments",children:"Diffusion Model Deployments"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"# Deploy FLUX.1 Schnell for image generation\nhelm install flux-diffusers ./blueprints/inference/inference-charts \\\n  --values ./blueprints/inference/inference-charts/values-flux-1-diffusers.yaml\n\n# Deploy Stable Diffusion XL\nhelm install sdxl-diffusers ./blueprints/inference/inference-charts \\\n  --values ./blueprints/inference/inference-charts/values-stable-diffusion-xl-base-1-diffusers.yaml\n"})}),"\n",(0,s.jsx)(n.h3,{id:"neuron-deployments",children:"Neuron Deployments"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"# Deploy Llama 3.1 8B on Inferentia\nhelm install llama31-neuron ./blueprints/inference/inference-charts \\\n  --values ./blueprints/inference/inference-charts/values-llama-31-8b-vllm-neuron.yaml\n\n# Deploy Llama 3 70B with Ray-VLLM on Inferentia\nhelm install llama3-70b-neuron ./blueprints/inference/inference-charts \\\n  --values ./blueprints/inference/inference-charts/values-llama-3-70b-ray-vllm-neuron.yaml\n"})}),"\n",(0,s.jsx)(n.h2,{id:"configuration",children:"Configuration"}),"\n",(0,s.jsx)(n.h3,{id:"key-parameters",children:"Key Parameters"}),"\n",(0,s.jsxs)(n.table,{children:[(0,s.jsx)(n.thead,{children:(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.th,{children:"Parameter"}),(0,s.jsx)(n.th,{children:"Description"}),(0,s.jsx)(n.th,{children:"Default"})]})}),(0,s.jsxs)(n.tbody,{children:[(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.code,{children:"inference.accelerator"})}),(0,s.jsxs)(n.td,{children:["Accelerator type (",(0,s.jsx)(n.code,{children:"gpu"})," or ",(0,s.jsx)(n.code,{children:"neuron"}),")"]}),(0,s.jsx)(n.td,{children:(0,s.jsx)(n.code,{children:"gpu"})})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.code,{children:"inference.framework"})}),(0,s.jsxs)(n.td,{children:["Framework (",(0,s.jsx)(n.code,{children:"vllm"}),", ",(0,s.jsx)(n.code,{children:"ray-vllm"}),", ",(0,s.jsx)(n.code,{children:"triton-vllm"}),", ",(0,s.jsx)(n.code,{children:"aibrix"}),", etc.)"]}),(0,s.jsx)(n.td,{children:(0,s.jsx)(n.code,{children:"vllm"})})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.code,{children:"inference.serviceName"})}),(0,s.jsx)(n.td,{children:"Name of the inference service"}),(0,s.jsx)(n.td,{children:(0,s.jsx)(n.code,{children:"inference"})})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.code,{children:"inference.modelServer.deployment.replicas"})}),(0,s.jsx)(n.td,{children:"Number of replicas"}),(0,s.jsx)(n.td,{children:(0,s.jsx)(n.code,{children:"1"})})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.code,{children:"model"})}),(0,s.jsx)(n.td,{children:"Model ID from Hugging Face Hub"}),(0,s.jsx)(n.td,{children:(0,s.jsx)(n.code,{children:"NousResearch/Llama-3.2-1B"})})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.code,{children:"modelParameters.gpuMemoryUtilization"})}),(0,s.jsx)(n.td,{children:"GPU memory utilization"}),(0,s.jsx)(n.td,{children:(0,s.jsx)(n.code,{children:"0.8"})})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.code,{children:"modelParameters.maxModelLen"})}),(0,s.jsx)(n.td,{children:"Maximum model sequence length"}),(0,s.jsx)(n.td,{children:(0,s.jsx)(n.code,{children:"8192"})})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.code,{children:"modelParameters.tensorParallelSize"})}),(0,s.jsx)(n.td,{children:"Tensor parallel size"}),(0,s.jsx)(n.td,{children:(0,s.jsx)(n.code,{children:"1"})})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.code,{children:"modelParameters.pipelineParallelSize"})}),(0,s.jsx)(n.td,{children:"Pipeline parallel size"}),(0,s.jsx)(n.td,{children:(0,s.jsx)(n.code,{children:"1"})})]})]})]}),"\n",(0,s.jsx)(n.h3,{id:"custom-configuration",children:"Custom Configuration"}),"\n",(0,s.jsx)(n.p,{children:"Create a custom values file:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-yaml",children:'inference:\n  accelerator: gpu  # or neuron\n  framework: vllm   # vllm, ray-vllm, triton-vllm, aibrix, lws-vllm, diffusers\n  serviceName: my-inference\n  modelServer:\n    deployment:\n      replicas: 1\n      instanceType: g5.2xlarge\n\nmodel: "NousResearch/Llama-3.2-1B"\nmodelParameters:\n  gpuMemoryUtilization: 0.8\n  maxModelLen: 8192\n  tensorParallelSize: 1\n'})}),"\n",(0,s.jsx)(n.p,{children:"Deploy with custom values:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"helm install my-inference ./blueprints/inference/inference-charts \\\n  --values custom-values.yaml\n"})}),"\n",(0,s.jsx)(n.h2,{id:"api-usage",children:"API Usage"}),"\n",(0,s.jsx)(n.p,{children:"The deployed services expose different API endpoints based on the framework:"}),"\n",(0,s.jsx)(n.h3,{id:"vllmray-vllm",children:"VLLM/Ray-VLLM"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.code,{children:"/v1/models"})," - List available models"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.code,{children:"/v1/chat/completions"})," - Chat completion API"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.code,{children:"/v1/completions"})," - Text completion API"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.code,{children:"/metrics"})," - Prometheus metrics"]}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"triton-vllm",children:"Triton-VLLM"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.code,{children:"/v2/models"})," - List available models"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.code,{children:"/v2/models/vllm_model/generate"})," - Model inference"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.code,{children:"/v2/health/ready"})," - Health checks"]}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"diffusers",children:"Diffusers"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.code,{children:"/v1/generations"})," - Image generation API"]}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"example-usage",children:"Example Usage"}),"\n",(0,s.jsx)(n.p,{children:"Access your service via port-forward:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"kubectl port-forward svc/<service-name> 8000\n"})}),"\n",(0,s.jsx)(n.p,{children:"Test the API:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:'# Chat completion (VLLM/Ray-VLLM)\ncurl -X POST http://localhost:8000/v1/chat/completions \\\n  -H "Content-Type: application/json" \\\n  -d \'{\n    "model": "your-model-name",\n    "messages": [{"role": "user", "content": "Hello!"}],\n    "max_tokens": 100\n  }\'\n\n# Image generation (Diffusers)\ncurl -X POST http://localhost:8000/v1/generations \\\n  -H \'Content-Type: application/json\' \\\n  -d \'{"prompt": "A beautiful sunset over mountains"}\'\n'})}),"\n",(0,s.jsx)(n.h2,{id:"troubleshooting",children:"Troubleshooting"}),"\n",(0,s.jsx)(n.h3,{id:"common-issues",children:"Common Issues"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Pod stuck in Pending state"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Check if GPU/Neuron nodes are available"}),"\n",(0,s.jsx)(n.li,{children:"Verify resource requests match available hardware"}),"\n",(0,s.jsx)(n.li,{children:"For LeaderWorkerSet deployments: Ensure LeaderWorkerSet CRD is installed"}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Model download failures"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:["Ensure Hugging Face token is correctly configured as secret ",(0,s.jsx)(n.code,{children:"hf-token"})]}),"\n",(0,s.jsx)(n.li,{children:"Check network connectivity to Hugging Face Hub"}),"\n",(0,s.jsx)(n.li,{children:"Verify model ID is correct and accessible"}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Out of memory errors"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:["Adjust ",(0,s.jsx)(n.code,{children:"gpuMemoryUtilization"})," parameter (try reducing from 0.8 to 0.7)"]}),"\n",(0,s.jsx)(n.li,{children:"Consider using tensor parallelism for larger models"}),"\n",(0,s.jsx)(n.li,{children:"For large models, use LeaderWorkerSet or Ray deployments with multiple GPUs"}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Ray deployment issues"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Ensure KubeRay infrastructure is installed"}),"\n",(0,s.jsx)(n.li,{children:"Check Ray cluster status and worker connectivity"}),"\n",(0,s.jsx)(n.li,{children:"Verify Ray version compatibility"}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Triton deployment issues"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Check Triton server logs for model loading errors"}),"\n",(0,s.jsx)(n.li,{children:"Verify model repository configuration"}),"\n",(0,s.jsx)(n.li,{children:"Ensure proper health check endpoints are accessible"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"logs",children:"Logs"}),"\n",(0,s.jsx)(n.p,{children:"Check deployment logs based on framework:"}),"\n",(0,s.jsx)(n.h3,{id:"check-logs",children:"Check Logs"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"# VLLM deployments\nkubectl logs -l app.kubernetes.io/component=<service-name>\n\n# Ray deployments\nkubectl logs -l ray.io/node-type=head\nkubectl logs -l ray.io/node-type=worker\n\n# LeaderWorkerSet deployments\nkubectl logs -l leaderworkerset.sigs.k8s.io/role=leader\n"})}),"\n",(0,s.jsx)(n.h2,{id:"next-steps",children:"Next Steps"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:["Explore ",(0,s.jsx)(n.a,{href:"/docs/category/gpu-inference-on-eks",children:"GPU-specific configurations"})," for GPU deployments"]}),"\n",(0,s.jsxs)(n.li,{children:["Learn about ",(0,s.jsx)(n.a,{href:"/docs/category/neuron-inference-on-eks",children:"Neuron-specific configurations"})," for Inferentia deployments"]}),"\n"]})]})}function h(e={}){const{wrapper:n}={...(0,i.R)(),...e.components};return n?(0,s.jsx)(n,{...e,children:(0,s.jsx)(t,{...e})}):t(e)}}}]);